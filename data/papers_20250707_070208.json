[
  {
    "title": "MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal\n  Policies in Real",
    "summary": "Robots must integrate multiple sensory modalities to act effectively in the\nreal world. Yet, learning such multimodal policies at scale remains\nchallenging. Simulation offers a viable solution, but while vision has\nbenefited from high-fidelity simulators, other modalities (e.g. sound) can be\nnotoriously difficult to simulate. As a result, sim-to-real transfer has\nsucceeded primarily in vision-based tasks, with multimodal transfer still\nlargely unrealized. In this work, we tackle these challenges by introducing\nMultiGen, a framework that integrates large-scale generative models into\ntraditional physics simulators, enabling multisensory simulation. We showcase\nour framework on the dynamic task of robot pouring, which inherently relies on\nmultimodal feedback. By synthesizing realistic audio conditioned on simulation\nvideo, our method enables training on rich audiovisual trajectories -- without\nany real robot data. We demonstrate effective zero-shot transfer to real-world\npouring with novel containers and liquids, highlighting the potential of\ngenerative modeling to both simulate hard-to-model modalities and close the\nmultimodal sim-to-real gap.",
    "authors": [
      "Renhao Wang",
      "Haoran Geng",
      "Tingle Li",
      "Feishi Wang",
      "Gopala Anumanchipalli",
      "Philipp Wu",
      "Trevor Darrell",
      "Boyi Li",
      "Pieter Abbeel",
      "Jitendra Malik",
      "Alexei A. Efros"
    ],
    "published": "2025-07-03T17:59:58Z",
    "updated": "2025-07-03T17:59:58Z",
    "arxiv_id": "2507.02864v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.02864v1.pdf",
    "ai_summary": "### Structured Summary of the Research Paper  \n\n**1. Main Research Contribution**  \nThe authors introduce **MultiGen**, a novel framework that bridges **multimodal sim-to-real transfer** by integrating **large-scale generative models** (e.g., text-to-speech, audio-visual synthesis) with traditional physics simulators. This enables the generation of **rich multisensory (audiovisual) training data** in simulation, addressing the challenge of simulating non-visual modalities like sound, which are critical for real-world robotic tasks but difficult to model accurately.\n\n---\n\n**2. Key Methodology/Approach**  \n- **Framework Design**: Combines **vision simulation** (using a physics engine for visual realism) with **generative models** for other modalities (e.g., audio).  \n  - **Physics-based visual simulation**: Generates realistic RGB-D video of the environment.  \n  - **Generative audio synthesis**: Trains a **large conditional audio generation model** (e.g., using simulated video data) to produce realistic sound effects (e.g., liquid pouring, object contact).  \n- **Policy Training**: Monomodal (vision-only) and multimodal (vision + audio) robotic policies are trained entirely in simulation using **self-supervised learning** on the synthesized audiovisual trajectories.  \n- **Zero-shot Transfer**: Evaluates the trained policies on real-world hardware without requiring any additional data, leveraging the generative models to close the sim-to-real gap for challenging modalities.  \n\n---\n\n**3. Primary Findings/Results**  \n- **Effective Sim-to-Real Transfer for Pouring Tasks**: MultiGen achieves high success rates in real-world robot pouring tasks using only simulation-trained policies (zero-shot transfer) for **novel containers and liquids** (not seen during training).  \n- **Improved Multimodal Learning**: Policies trained with synthesized audio (using TTS models) outperform those trained with physics-based audio only, as the generative model better captures the **rich variability of real-world acoustic feedback** (e.g., splashing, pouring sounds).  \n- **Generalization Across Modalities**: The framework demonstrates that integrating generative models for multimodal data (vision + audio) enables robust generalization to real-world scenarios, even when the physical dynamics of the simulator are not perfectly aligned with reality.  \n\n---\n\n**4. Potential Applications**  \n- **Multimodal Robotics**: Tasks requiring audio-visual integration, such as object manipulation (e.g., identifying materials by touch/sight/sound), grasping, or human-robot interaction (e.g., responding to verbal cues and visual context).  \n- **Sim-to-Real Training for Complex Environments**: Enabling large-scale policy learning for scenarios where collecting real-world data (especially non-visual modalities) is impractical or expensive (e.g., hazardous or acoustically complex environments).  \n- **Cross-Modal Sensing**: Enhancing robotic perception by simulating interactions between modalities (e.g., using audio to infer liquid levels indirectly when visual occlusion occurs).  \n- **Education and Training**: Creating lifelike simulation environments for training robots in human-centric settings (e.g., cooking, cleaning) where multimodal feedback (sound, motion) improves task understanding and adaptability.  \n\n---\n\nThis approach highlights the promise of **generative modeling** to augment physics-based simulators, enabling scalable, multimodal learning for real-world robotics without relying on costly real-data collection."
  }
]