[
  {
    "title": "MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal\n  Policies in Real",
    "summary": "Robots must integrate multiple sensory modalities to act effectively in the\nreal world. Yet, learning such multimodal policies at scale remains\nchallenging. Simulation offers a viable solution, but while vision has\nbenefited from high-fidelity simulators, other modalities (e.g. sound) can be\nnotoriously difficult to simulate. As a result, sim-to-real transfer has\nsucceeded primarily in vision-based tasks, with multimodal transfer still\nlargely unrealized. In this work, we tackle these challenges by introducing\nMultiGen, a framework that integrates large-scale generative models into\ntraditional physics simulators, enabling multisensory simulation. We showcase\nour framework on the dynamic task of robot pouring, which inherently relies on\nmultimodal feedback. By synthesizing realistic audio conditioned on simulation\nvideo, our method enables training on rich audiovisual trajectories -- without\nany real robot data. We demonstrate effective zero-shot transfer to real-world\npouring with novel containers and liquids, highlighting the potential of\ngenerative modeling to both simulate hard-to-model modalities and close the\nmultimodal sim-to-real gap.",
    "authors": [
      "Renhao Wang",
      "Haoran Geng",
      "Tingle Li",
      "Feishi Wang",
      "Gopala Anumanchipalli",
      "Philipp Wu",
      "Trevor Darrell",
      "Boyi Li",
      "Pieter Abbeel",
      "Jitendra Malik",
      "Alexei A. Efros"
    ],
    "published": "2025-07-03T17:59:58Z",
    "updated": "2025-07-03T17:59:58Z",
    "arxiv_id": "2507.02864v1",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2507.02864v1.pdf",
    "ai_summary": "### 1. **Main Research Contribution**  \nThe paper introduces **MultiGen**, a novel framework that bridges the gap between simulated and real-world multimodal policy learning by integrating **large-scale generative models** with traditional physics-based simulators. This approach enables the synthesis of **realistic audiovisual data** in simulation, addressing the challenge of sim-to-real transfer for tasks requiring integration of hard-to-simulate modalities (e.g., sound) alongside vision. The work demonstrates a scalable way to train robot policies in simulation using multimodal data, achieving **zero-shot generalization** to real-world settings without requiring real-world training data.  \n\n---\n\n### 2. **Key Methodology/Approach**  \n- **Framework Design**:  \n  - Combines **physics simulators** (for generating visual (video) and kinematic data) with **multimodal generative models** (for synthesizing high-quality, realistic audio).  \n  - Uses a **two-step pipeline**:  \n    1. **Simulated Video Generation**: A high-fidelity physics simulator (e.g., for pouring tasks) generates visual trajectories.  \n    2. **Audio Synthesis**: A pre-trained **generative audio model** (e.g., diffusion models) creates audio signals conditioned on the simulated video, enabling audiovisual (AV) data synthesis.  \n  - Trains **multimodal policies** in simulation using the synthesized AV data, with no reliance on real-world observations during training.  \n\n- **Technical Innovations**:  \n  - **Cross-modal Conditioning**: Leverages visual data from simulators as input for audio generation to ensure sensory coherence (e.g., sound of liquid splashing matches visual pouring dynamics).  \n  - **Policy Training**: Reinforcement learning (RL) policies are trained on the simulated AV data to perform tasks (e.g., pouring liquids), enabling the use of perception and control systems grounded in both vision and sound.  \n\n- **No Real-World Data Requirement**: The method does not use real-world AV data during training, relying solely on synthetic data to close the sim-to-real gap.  \n\n---\n\n### 3. **Primary Findings/Results**  \n- **Zero-Shot Transfer Success**:  \n  - The audio-conditioned policies trained in simulation **generalize to real robotic systems** performing pouring tasks with **novel containers and liquids** (e.g., rigid, viscous, or acoustically different materials).  \n  - Performance on real hardware is comparable to systems trained with real-world data, indicating that the synthetic AV data effectively captures real-world multimodal dynamics.  \n\n- **Realism of Synthesized Audio**:  \n  - The generative audio models produce **high-fidelity sounds** that, when paired with simulated video, enable the policy to learn meaningful cues (e.g., timing of splashes, container material interactions).  \n\n- **Multimodal Policy Benefits**:  \n  - Policies using both visual *and* auditive feedback outperform vision-only policies in simulation and real-world transfer, particularly in tasks with dynamic, contact-rich interactions.  \n\n- **Scalability**:  \n  - The framework is applicable to other multimodal tasks beyond pouring, suggesting broad potential for sim-to-real policy learning.  \n\n---\n\n### 4. **Potential Applications**  \n- **Robotic Manipulation**: Tasks requiring tactile/auditive feedback (e.g., grasping, object sorting, or cooking) where real-world data is costly to collect.  \n- **Sim-to-Real for Complex Sensory Tasks**: Training policies for environments where simulating specific modalities (sound, touch) has been historically difficult.  \n- **Data Diversity and Generalization**: Enabling robots to handle real-world variability (e.g., untrained objects, unstructured scenes) by synthesizing diverse AV scenarios in simulation.  \n- **Affordable Pre-Training**: A cost-effective alternative to real-world data collection for training robust, multimodal policies in resource-constrained settings.  \n- **Cross-Modality Research**: Advancing studies on how different sensory modalities interact, such as using audio to enhance visual perception or vice versa in robotic planning.  \n\n---\n\n### Summary  \n**MultiGen** addresses the challenge of sim-to-real transfer for multimodal policies by using physics simulators (for vision) and generative models (for sound) in tandem. A pre-trained audio model synthesizes realistic sounds from simulated video, enabling the training of policies that integrate vision and auditory feedback. This approach achieves zero-shot transfer to real-world dynamical tasks (e.g., pouring) and opens possibilities for scalable multimodal training without real data. Applications span robotics, affordable simulation-heavy research, and tasks requiring integration of under-modeled sensory inputs like touch or sound."
  }
]