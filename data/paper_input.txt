**Summary of "MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real"**

Robots require integration of multiple sensory inputs (e.g., vision, sound) to perform complex real-world tasks effectively. While high-fidelity simulations have advanced vision-based policy learning, modalities like audio remain difficult to model, limiting the success of sim-to-real transfer for multimodal tasks. The paper introduces **MultiGen**, a framework that combines traditional physics-based simulators with large-scale generative models to simulate multisensory environments. By synthesizing realistic audio from simulated video, MultiGen creates rich audiovisual training data without relying on real-world examples. 

The approach is validated on a dynamic task—robot pouring—which depends on both visual observation and auditory feedback (e.g., liquid sloshing sounds). MultiGen enables the robot to learn a policy using only simulated, multimodal data. Crucially, this learned policy achieves **zero-shot transfer** to real-world scenarios involving novel containers and liquids, demonstrating that generative models can effectively simulate hard-to-model sensory modalities. The results highlight the potential of MultiGen to bridge the sim-to-real gap for multimodal policies, offering a scalable solution to train robots using diverse sensory feedback in simulation before deployment in the real world.