# ğŸ” AutoRAG-Agent

AutoRAG-Agent is a **modular Retrieval-Augmented Generation (RAG) pipeline** designed for document ingestion, semantic search, and answer generation using Large Language Models (LLMs).

It supports:
- âœ… PDF & Web Document Ingestion
- âœ… Text Cleaning & Chunking
- âœ… Embedding + Vector Indexing (FAISS)
- âœ… Semantic Search
- âœ… Answer Generation via LLMs  
With **GPU acceleration** (works in **Google Colab Free Tier** too).

âœ… Built for ease of use in **Google Colab**.

---

## ğŸš€ Try it on Colab

You can run and test this project directly on Google Colab by clicking below:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1HHCyOO93v13nA8JTJagmiDW1kVIvkErX#scrollTo=EbZq5wYRNx8i)

---
---

## ğŸ“‚ Directory Structure
/*
Project Directory Structure: AutoRAG-Agent

/*
  ğŸ“‚ Project Structure â€” AutoRAG-Agent
  
  AutoRAG-Agent/               // Root project directory
  
    â”œâ”€â”€ app/                  // Core modules for RAG pipeline
    â”‚   â”œâ”€â”€ ingestion.py          // Document ingestion: PDFs, URLs, and raw text
    â”‚   â”œâ”€â”€ preprocessing.py      // Text cleaning and chunking (LangChain splitter)
    â”‚   â”œâ”€â”€ embedding.py          // Embedding text chunks + FAISS vector indexing
    â”‚   â”œâ”€â”€ query.py              // Semantic search over FAISS index
    â”‚   â””â”€â”€ answering.py          // Answer generation via LLM (Flan-T5 or similar)
    â”‚
    â”œâ”€â”€ sample_data/          // Sample PDF documents for testing
    â”‚   â””â”€â”€ sample_doc.pdf        // Example PDF downloaded from arXiv
    â”‚
    â”œâ”€â”€ models/               // (Optional) Fine-tuned or downloaded models (future use)
    â”‚
    â”œâ”€â”€ ui/                   // (Optional) UI-related code like Gradio, Streamlit, etc.
    â”‚
    â””â”€â”€ README.md             // Project documentation (this file)

  ğŸ’¡ Explanation:
  - `app/` contains all the core modules of the RAG pipeline:
    * ingestion.py â†’ Loads and extracts text from PDFs, URLs, or raw text.
    * preprocessing.py â†’ Cleans and chunks text for embeddings.
    * embedding.py â†’ Embeds text using Sentence Transformers and stores vectors in FAISS.
    * query.py â†’ Performs semantic similarity search over FAISS index.
    * answering.py â†’ Uses a lightweight LLM to answer user queries from retrieved chunks.

  - `sample_data/` contains sample PDFs (like arXiv papers) for testing the pipeline.

  - `models/` can be used to store fine-tuned models or downloaded checkpoints later.

  - `ui/` is reserved for optional future user interfaces (e.g., Gradio or Streamlit apps).

  - The pipeline is modular and ready to be run in notebooks or scripts.
  
  âœ… Clean, GPU-accelerated, modular pipeline ready for research, testing, and integration.


---

## ğŸš€ Features & Pipeline Workflow

| Step              | Description |
|-------------------|-------------|
| Ingestion         | Load text from PDFs, URLs, or raw text |
| Preprocessing     | Clean and split text into chunks |
| Embedding & Indexing | Embed text chunks with Sentence Transformers and store in FAISS index |
| Semantic Search   | Search FAISS index for relevant chunks |
| Answer Generation | Generate answers using lightweight LLMs (Flan-T5) |

---

## ğŸ“‹ Setup & Requirements (Google Colab Compatible)


!pip install pypdf requests beautifulsoup4 langchain sentence-transformers faiss-gpu transformers
Go to Runtime â†’ Change runtime type â†’ Select GPU (T4/A100 recommended).

This setup uses faiss-gpu for fast similarity search on GPU.


ğŸ“ Module Details & Functionality
1. app/ingestion.py
Ingest text from:

PDF files (pypdf)

URLs (requests + BeautifulSoup)

Raw text strings

2. app/preprocessing.py
Clean text (remove line breaks and tabs).

Chunk text using LangChain's RecursiveCharacterTextSplitter.

3. app/embedding.py
Embed text chunks using sentence-transformers/all-MiniLM-L6-v2.

Store embeddings in FAISS index for fast retrieval.

4. app/query.py
Perform semantic search on FAISS index to retrieve relevant chunks.

5. app/answering.py
Generate answers using lightweight LLMs like google/flan-t5-small.

âš ï¸ Notes
Adjust chunk size & overlap in preprocessing.py for large documents.

Easily swap out LLMs and Embedding Models (just change model names).

This pipeline is ready for integration with Gradio/FastAPI for interactive UI.

ğŸ“ˆ Future Improvements
FAISS index saving/loading from disk

Multi-document ingestion support

Web-based or Command-line UI

Support for advanced LLMs like Mistral, Llama 3, etc.

ğŸ“š References
LangChain

FAISS

Sentence Transformers

Transformers (Hugging Face)

ğŸ¤ License
MIT License

âœ… Conclusion
AutoRAG-Agent is a simple, modular, and GPU-accelerated RAG pipeline for document ingestion, semantic search, and LLM-based answering, ready for both research and practical use cases.
